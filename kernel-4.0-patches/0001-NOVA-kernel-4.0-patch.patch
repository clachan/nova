From e813a1db7b6853f1d18732b31029f93e10901ba6 Mon Sep 17 00:00:00 2001
From: Andiry Xu <jix024@cs.ucsd.edu>
Date: Sun, 7 Jun 2015 17:51:38 -0700
Subject: [PATCH 1/2] NOVA kernel 4.0 patch

This patch adds NOVA support to Linux kernel 4.0.
---
 arch/x86/include/asm/io.h     |   5 ++
 arch/x86/mm/ioremap.c         |  46 +++++++++++++++--
 arch/x86/mm/pat.c             |   3 ++
 arch/x86/mm/pgtable.c         |   2 +
 include/asm-generic/pgtable.h |   8 +++
 include/linux/io.h            |  11 ++++
 include/linux/mm.h            |  13 +++++
 include/linux/vmalloc.h       |   2 +-
 lib/ioremap.c                 | 117 +++++++++++++++++++++++++++++++++++++-----
 mm/madvise.c                  |   6 +++
 mm/memory.c                   | 106 ++++++++++++++++++++++++++++++++++++++
 mm/mlock.c                    |   5 ++
 mm/mmap.c                     |   4 ++
 mm/mprotect.c                 |   3 ++
 mm/vmalloc.c                  |  10 +++-
 15 files changed, 322 insertions(+), 19 deletions(-)

diff --git a/arch/x86/include/asm/io.h b/arch/x86/include/asm/io.h
index 34a5b93..559b4c1 100644
--- a/arch/x86/include/asm/io.h
+++ b/arch/x86/include/asm/io.h
@@ -178,9 +178,14 @@ static inline unsigned int isa_virt_to_bus(volatile void *address)
  */
 extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_cache_ro(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size,
 				unsigned long prot_val);
 
+extern void __iomem *ioremap_hpage_cache_ro(resource_size_t offset,
+		unsigned long size);
+extern void __iomem *ioremap_hpage_cache(resource_size_t offset,
+		unsigned long size);
 /*
  * The default ioremap() behavior is non-cached:
  */
diff --git a/arch/x86/mm/ioremap.c b/arch/x86/mm/ioremap.c
index fdf617c..cdde3af 100644
--- a/arch/x86/mm/ioremap.c
+++ b/arch/x86/mm/ioremap.c
@@ -21,6 +21,7 @@
 #include <asm/tlbflush.h>
 #include <asm/pgalloc.h>
 #include <asm/pat.h>
+#include <asm/cpufeature.h>
 
 #include "physaddr.h"
 
@@ -74,8 +75,9 @@ static int __ioremap_check_ram(unsigned long start_pfn, unsigned long nr_pages,
  * have to convert them into an offset in a page-aligned mapping, but the
  * caller shouldn't need to know that small detail.
  */
-static void __iomem *__ioremap_caller(resource_size_t phys_addr,
-		unsigned long size, enum page_cache_mode pcm, void *caller)
+static void __iomem *___ioremap_caller(resource_size_t phys_addr,
+		unsigned long size, enum page_cache_mode pcm, void *caller,
+		unsigned int hpages, unsigned int readonly)
 {
 	unsigned long offset, vaddr;
 	resource_size_t pfn, last_pfn, last_addr;
@@ -171,6 +173,10 @@ static void __iomem *__ioremap_caller(resource_size_t phys_addr,
 		break;
 	}
 
+	/* Map pages RO */
+	if (readonly)
+		prot = __pgprot((unsigned long)prot.pgprot & ~_PAGE_RW);
+
 	/*
 	 * Ok, go for it..
 	 */
@@ -183,8 +189,13 @@ static void __iomem *__ioremap_caller(resource_size_t phys_addr,
 	if (kernel_map_sync_memtype(phys_addr, size, pcm))
 		goto err_free_area;
 
-	if (ioremap_page_range(vaddr, vaddr + size, phys_addr, prot))
-		goto err_free_area;
+	if (hpages) {
+		if (ioremap_hpage_range(vaddr, vaddr + size, phys_addr, prot))
+			goto err_free_area;
+	} else {
+		if (ioremap_page_range(vaddr, vaddr + size, phys_addr, prot))
+			goto err_free_area;
+	}
 
 	ret_addr = (void __iomem *) (vaddr + offset);
 	mmiotrace_ioremap(unaligned_phys_addr, unaligned_size, ret_addr);
@@ -204,6 +215,12 @@ err_free_memtype:
 	return NULL;
 }
 
+static void __iomem *__ioremap_caller(resource_size_t phys_addr,
+	unsigned long size, unsigned long prot_val, void *caller)
+{
+	return ___ioremap_caller(phys_addr, size, prot_val, caller, 0, 0);
+}
+
 /**
  * ioremap_nocache     -   map bus memory into CPU space
  * @phys_addr:    bus address of the memory
@@ -261,6 +278,20 @@ void __iomem *ioremap_wc(resource_size_t phys_addr, unsigned long size)
 }
 EXPORT_SYMBOL(ioremap_wc);
 
+void __iomem *ioremap_hpage_cache(resource_size_t phys_addr, unsigned long size)
+{
+	return ___ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,
+				__builtin_return_address(0), 1, 0);
+}
+EXPORT_SYMBOL(ioremap_hpage_cache);
+
+void __iomem *ioremap_hpage_cache_ro(resource_size_t phys_addr, unsigned long size)
+{
+	return ___ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,
+				__builtin_return_address(0), 1, 1);
+}
+EXPORT_SYMBOL(ioremap_hpage_cache_ro);
+
 void __iomem *ioremap_cache(resource_size_t phys_addr, unsigned long size)
 {
 	return __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,
@@ -268,6 +299,13 @@ void __iomem *ioremap_cache(resource_size_t phys_addr, unsigned long size)
 }
 EXPORT_SYMBOL(ioremap_cache);
 
+void __iomem *ioremap_cache_ro(resource_size_t phys_addr, unsigned long size)
+{
+	return ___ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,
+				__builtin_return_address(0), 0, 1);
+}
+EXPORT_SYMBOL(ioremap_cache_ro);
+
 void __iomem *ioremap_prot(resource_size_t phys_addr, unsigned long size,
 				unsigned long prot_val)
 {
diff --git a/arch/x86/mm/pat.c b/arch/x86/mm/pat.c
index 7ac6869..91a9630 100644
--- a/arch/x86/mm/pat.c
+++ b/arch/x86/mm/pat.c
@@ -304,6 +304,9 @@ static int pat_pagerange_is_ram(resource_size_t start, resource_size_t end)
 	unsigned long end_pfn = (end + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	struct pagerange_state state = {start_pfn, 0, 0};
 
+	if (start_pfn >= max_pfn)
+		return 0;
+
 	/*
 	 * For legacy reasons, physical address range in the legacy ISA
 	 * region is tracked as non-RAM. This will allow users of
diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
index 7b22ada..8045a5f 100644
--- a/arch/x86/mm/pgtable.c
+++ b/arch/x86/mm/pgtable.c
@@ -1,5 +1,6 @@
 #include <linux/mm.h>
 #include <linux/gfp.h>
+#include <linux/export.h>
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
 #include <asm/tlb.h>
@@ -343,6 +344,7 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 
 	return changed;
 }
+EXPORT_SYMBOL(ptep_set_access_flags);
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 int pmdp_set_access_flags(struct vm_area_struct *vma,
diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
index 4d46085..a31b042 100644
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@ -323,6 +323,10 @@ static inline int pud_none_or_clear_bad(pud_t *pud)
 {
 	if (pud_none(*pud))
 		return 1;
+	if (unlikely(pud_large(*pud))) {
+		pud_clear(pud);
+		return 1;
+	}
 	if (unlikely(pud_bad(*pud))) {
 		pud_clear_bad(pud);
 		return 1;
@@ -334,6 +338,10 @@ static inline int pmd_none_or_clear_bad(pmd_t *pmd)
 {
 	if (pmd_none(*pmd))
 		return 1;
+	if (unlikely(pmd_large(*pmd))) {
+		pmd_clear(pmd);
+		return 1;
+	}
 	if (unlikely(pmd_bad(*pmd))) {
 		pmd_clear_bad(pmd);
 		return 1;
diff --git a/include/linux/io.h b/include/linux/io.h
index fa02e55..b89ab00 100644
--- a/include/linux/io.h
+++ b/include/linux/io.h
@@ -38,6 +38,17 @@ static inline int ioremap_page_range(unsigned long addr, unsigned long end,
 }
 #endif
 
+#ifdef CONFIG_MMU
+int ioremap_hpage_range(unsigned long addr, unsigned long end,
+		       phys_addr_t phys_addr, pgprot_t prot);
+#else
+static inline int ioremap_hpage_range(unsigned long addr, unsigned long end,
+				     phys_addr_t phys_addr, pgprot_t prot)
+{
+	return 0;
+}
+#endif
+
 /*
  * Managed iomap interface
  */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 47a9392..4e83928 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -123,6 +123,7 @@ extern unsigned int kobjsize(const void *objp);
 #define VM_MAYSHARE	0x00000080
 
 #define VM_GROWSDOWN	0x00000100	/* general info on the segment */
+#define	VM_XIP_HUGETLB	0x00000200
 #define VM_PFNMAP	0x00000400	/* Page-ranges managed without "struct page", just pure PFN */
 #define VM_DENYWRITE	0x00000800	/* ETXTBSY on write attempts.. */
 
@@ -212,6 +213,11 @@ extern pgprot_t protection_map[16];
 #define FAULT_FLAG_TRIED	0x20	/* Second try */
 #define FAULT_FLAG_USER		0x40	/* The fault originated in userspace */
 
+static inline int is_xip_hugetlb_mapping(struct vm_area_struct *vma)
+{
+	return !!(vma->vm_flags & VM_XIP_HUGETLB);
+}
+
 /*
  * vm_fault is filled by the the pagefault handler and passed to the vma's
  * ->fault function. The vma's ->fault is responsible for returning a bitmask
@@ -1254,6 +1260,13 @@ static inline int fixup_user_fault(struct task_struct *tsk,
 }
 #endif
 
+extern pte_t *pte_alloc_pagesz(struct mm_struct *mm, unsigned long addr,
+							unsigned long sz);
+extern pte_t *pte_offset_pagesz(struct mm_struct *mm, unsigned long addr,
+							unsigned long *sz);
+extern void unmap_xip_hugetlb_range(struct vm_area_struct *vma,
+					unsigned long start, unsigned long end);
+
 extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);
 extern int access_remote_vm(struct mm_struct *mm, unsigned long addr,
 		void *buf, int len, int write);
diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 0ec5983..2e2a6bd 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -25,7 +25,7 @@ struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
  * Can be overriden by arch-specific value.
  */
 #ifndef IOREMAP_MAX_ORDER
-#define IOREMAP_MAX_ORDER	(7 + PAGE_SHIFT)	/* 128 pages */
+#define IOREMAP_MAX_ORDER	(PUD_SHIFT)	/* 1G pages */
 #endif
 
 struct vm_struct {
diff --git a/lib/ioremap.c b/lib/ioremap.c
index 0c9216c..69d6e0b 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -12,6 +12,7 @@
 #include <linux/export.h>
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
+#include <asm/tlbflush.h>
 
 static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
@@ -32,37 +33,96 @@ static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
 }
 
 static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
-		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot,
+		int hpage)
 {
-	pmd_t *pmd;
+	pmd_t *pmd_page, *pmd;
 	unsigned long next;
 
 	phys_addr -= addr;
-	pmd = pmd_alloc(&init_mm, pud, addr);
-	if (!pmd)
+	pmd_page = pmd_alloc(&init_mm, pud, addr);
+	if (!pmd_page)
 		return -ENOMEM;
+
+	if (hpage)
+		printk(KERN_INFO "PMD_MAPPING (START) [%s, %d] "
+			"VA START 0x%lx, VA_END 0x%lx, PA 0x%lx, SIZE 0x%lx\n",
+			__func__, __LINE__, addr, end,
+			(unsigned long)(phys_addr + addr), (end - addr));
+
+	pmd = pmd_page;
 	do {
 		next = pmd_addr_end(addr, end);
-		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
-			return -ENOMEM;
+		if (hpage && cpu_has_pse && ((next - addr) >= PMD_SIZE)
+				&& IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {
+			u64 pfn = ((u64)(phys_addr + addr)) >> PAGE_SHIFT;
+			prot = __pgprot((unsigned long)prot.pgprot | _PAGE_PSE);
+			if ((s64)pfn < 0)
+			{
+				printk (KERN_INFO "MAPPING ERROR [%s, %d]: "
+					"phys_addr 0x%lx addr 0x%lx, next 0x%lx, "
+					"end 0x%lx, pfn 0x%lx\n", __func__,
+					__LINE__, (unsigned long)phys_addr,
+					(unsigned long)addr, (unsigned long)next,
+					(unsigned long)end, (unsigned long)pfn);
+				return -ENOMEM;
+			}
+
+			spin_lock(&init_mm.page_table_lock);
+			set_pte((pte_t *)pmd, pfn_pte(pfn, prot));
+			spin_unlock(&init_mm.page_table_lock);
+		} else {
+			if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
+				return -ENOMEM;
+		}
 	} while (pmd++, addr = next, addr != end);
 	return 0;
 }
 
 static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
-		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot,
+		int hpage)
 {
-	pud_t *pud;
+	pud_t *pud_page, *pud;
 	unsigned long next;
 
 	phys_addr -= addr;
-	pud = pud_alloc(&init_mm, pgd, addr);
-	if (!pud)
+	pud_page = pud_alloc(&init_mm, pgd, addr);
+	if (!pud_page)
 		return -ENOMEM;
+
+	if (hpage)
+		printk(KERN_INFO "PUD_MAPPING (START) [%s, %d] "
+			"VA START 0x%lx, VA_END 0x%lx, PA 0x%lx, SIZE 0x%lx\n",
+			__func__, __LINE__, addr, end,
+			(unsigned long)(phys_addr + addr), (end - addr));
+
+	pud = pud_page;
 	do {
 		next = pud_addr_end(addr, end);
-		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))
-			return -ENOMEM;
+		if (hpage && cpu_has_gbpages && ((next - addr) >= PUD_SIZE)
+				&& IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {
+			u64 pfn = ((u64)(phys_addr + addr)) >> PAGE_SHIFT;
+			prot = __pgprot((unsigned long)prot.pgprot | _PAGE_PSE);
+			if ((s64)pfn < 0)
+			{
+				printk (KERN_INFO "MAPPING ERROR [%s, %d]: "
+					"phys_addr 0x%lx addr 0x%lx, next 0x%lx, "
+					"end 0x%lx, pfn 0x%lx\n", __func__,
+					__LINE__, (unsigned long)phys_addr,
+					(unsigned long)addr, (unsigned long)next,
+					(unsigned long)end, (unsigned long)pfn);
+				return -ENOMEM;
+			}
+
+			spin_lock(&init_mm.page_table_lock);
+			set_pte((pte_t *)pud, pfn_pte(pfn, prot));
+			spin_unlock(&init_mm.page_table_lock);
+		} else {
+			if (ioremap_pmd_range(pud, addr, next, phys_addr + addr,
+						prot, hpage))
+				return -ENOMEM;
+		}
 	} while (pud++, addr = next, addr != end);
 	return 0;
 }
@@ -82,7 +142,7 @@ int ioremap_page_range(unsigned long addr,
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
-		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot);
+		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot, 0);
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
@@ -92,3 +152,34 @@ int ioremap_page_range(unsigned long addr,
 	return err;
 }
 EXPORT_SYMBOL_GPL(ioremap_page_range);
+
+int ioremap_hpage_range(unsigned long addr,
+		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+{
+	pgd_t *pgd;
+	unsigned long start;
+	unsigned long next;
+	int err;
+
+	BUG_ON(addr >= end);
+
+	printk(KERN_INFO "[%s, %d] hpages ON: startVA 0x%lx, endVA 0x%lx, "
+		"startPA 0x%lx, startPFSN 0x%lx\n", __func__, __LINE__,
+		addr, end, (unsigned long)phys_addr,
+		(unsigned long)phys_addr >> PAGE_SHIFT);
+
+	start = addr;
+	phys_addr -= addr;
+	pgd = pgd_offset_k(addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot, 1);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+
+	flush_cache_vmap(start, end);
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(ioremap_hpage_range);
diff --git a/mm/madvise.c b/mm/madvise.c
index d551475..683e6b4 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -510,6 +510,12 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 		if (!vma)
 			goto out;
 
+		/* madvise not supported with XIP_HUGE_TLB */
+		if (is_xip_hugetlb_mapping(vma)) {
+			error = -EINVAL;
+			goto out;
+		}
+
 		/* Here start < (end|vma->vm_end). */
 		if (start < vma->vm_start) {
 			unmapped_error = -ENOMEM;
diff --git a/mm/memory.c b/mm/memory.c
index 97839f5..79cdfe0 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1025,6 +1025,9 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			!vma->anon_vma)
 		return 0;
 
+	if (is_xip_hugetlb_mapping(vma))
+		return 0;
+
 	if (is_vm_hugetlb_page(vma))
 		return copy_hugetlb_page_range(dst_mm, src_mm, vma);
 
@@ -1307,6 +1310,9 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 				__unmap_hugepage_range_final(tlb, vma, start, end, NULL);
 				i_mmap_unlock_write(vma->vm_file->f_mapping);
 			}
+		} else if (is_xip_hugetlb_mapping(vma)) {
+			unmap_xip_hugetlb_range(vma, start, end);
+			start = end;
 		} else
 			unmap_page_range(tlb, vma, start, end, details);
 	}
@@ -3295,6 +3301,18 @@ int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	/* do counter updates before entering really critical section. */
 	check_sync_rss_stat(current);
 
+	if (is_xip_hugetlb_mapping(vma)) {
+		int err;
+		struct vm_fault vmf;
+		vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+		vmf.pgoff = (((address & PAGE_MASK) - vma->vm_start) >> PAGE_SHIFT);
+		vmf.flags = flags;
+		vmf.page = NULL;
+		err = vma->vm_ops->fault(vma, &vmf);
+		if (!err || (err == VM_FAULT_NOPAGE))
+			return 0;
+	}
+
 	/*
 	 * Enable the memcg OOM handling for faults triggered in user
 	 * space.  Kernel faults are handled more gracefully.
@@ -3375,6 +3393,94 @@ int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 }
 #endif /* __PAGETABLE_PMD_FOLDED */
 
+/****************************************************************************/
+/* XIP_HUGETLB support */
+pte_t *pte_offset_pagesz(struct mm_struct *mm, unsigned long addr,
+						unsigned long *sz)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_present(*pgd)) {
+		*sz = PGDIR_SIZE;
+		return (pte_t *)pgd;
+	}
+
+	pud = pud_offset(pgd, addr);
+	if (pud_none(*pud) || pud_large(*pud)) {
+		*sz = PUD_SIZE;
+		return (pte_t *)pud;
+	}
+	pmd = pmd_offset(pud, addr);
+	//if (pmd_none(*pmd) || pmd_large(*pmd)) {
+	*sz = PMD_SIZE;
+	return (pte_t *)pmd;
+}
+EXPORT_SYMBOL(pte_offset_pagesz);
+
+pte_t *pte_alloc_pagesz(struct mm_struct *mm, unsigned long addr,
+						unsigned long sz)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (pud) {
+		if (sz == PUD_SIZE) {
+			pte = (pte_t *)pud;
+		} else {
+			BUG_ON(sz != PMD_SIZE);
+			pte = (pte_t *) pmd_alloc(mm, pud, addr);
+		}
+	}
+	BUG_ON(pte && !pte_none(*pte) && !pte_huge(*pte));
+
+	return pte;
+}
+EXPORT_SYMBOL(pte_alloc_pagesz);
+
+static void __unmap_xip_hugetlb_range(struct vm_area_struct *vma,
+				unsigned long start, unsigned long end)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long address;
+	pte_t *ptep;
+	pte_t pte;
+	unsigned long sz;
+
+	WARN_ON(!is_xip_hugetlb_mapping(vma));
+
+	mmu_notifier_invalidate_range_start(mm, start, end);
+	spin_lock(&mm->page_table_lock);
+	for (address = start, sz=PMD_SIZE; address < end; address += sz) {
+		ptep = pte_offset_pagesz(mm, address, &sz);
+		if (!ptep)
+			continue;
+
+		pte = ptep_get_and_clear(mm, address, ptep);
+		if (pte_none(pte))
+			continue;
+	}
+	flush_tlb_range(vma, start, end);
+	spin_unlock(&mm->page_table_lock);
+	mmu_notifier_invalidate_range_end(mm, start, end);
+}
+
+void unmap_xip_hugetlb_range(struct vm_area_struct *vma,
+				unsigned long start, unsigned long end)
+{
+	i_mmap_lock_write(vma->vm_file->f_mapping);
+	__unmap_xip_hugetlb_range(vma, start, end);
+	i_mmap_unlock_write(vma->vm_file->f_mapping);
+}
+EXPORT_SYMBOL(unmap_xip_hugetlb_range);
+
+/****************************************************************************/
+
 static int __follow_pte(struct mm_struct *mm, unsigned long address,
 		pte_t **ptepp, spinlock_t **ptlp)
 {
diff --git a/mm/mlock.c b/mm/mlock.c
index 8a54cd2..2ca5748 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -237,6 +237,11 @@ long __mlock_vma_pages_range(struct vm_area_struct *vma,
 	VM_BUG_ON_VMA(end   > vma->vm_end, vma);
 	VM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_sem), mm);
 
+	if (is_xip_hugetlb_mapping(vma)) {
+		vma->vm_flags &= ~VM_LOCKED;
+		return nr_pages;
+	}
+
 	gup_flags = FOLL_TOUCH | FOLL_MLOCK;
 	/*
 	 * We want to touch writable mappings with a write fault in order
diff --git a/mm/mmap.c b/mm/mmap.c
index 9ec50a3..d9e0bce 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1791,6 +1791,7 @@ found:
 	VM_BUG_ON(gap_start + info->length > gap_end);
 	return gap_start;
 }
+EXPORT_SYMBOL(unmapped_area);
 
 unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 {
@@ -2444,6 +2445,9 @@ static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	struct vm_area_struct *new;
 	int err = -ENOMEM;
 
+	if (is_xip_hugetlb_mapping(vma))
+		return -EINVAL;
+
 	if (is_vm_hugetlb_page(vma) && (addr &
 					~(huge_page_mask(hstate_vma(vma)))))
 		return -EINVAL;
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 8858483..0d60e66 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -269,6 +269,9 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 		return 0;
 	}
 
+	if (is_xip_hugetlb_mapping(vma))
+		return -EINVAL;
+
 	/*
 	 * If we make a private mapping writable we increase our commit;
 	 * but (without finer accounting) cannot reduce our commit if we
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 49abccf..6c645b7 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -74,6 +74,10 @@ static void vunmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end)
 	pmd = pmd_offset(pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
+		if (pmd_large(*pmd)) {
+			pmd_clear(pmd);
+			continue;
+		}
 		if (pmd_none_or_clear_bad(pmd))
 			continue;
 		vunmap_pte_range(pmd, addr, next);
@@ -88,6 +92,10 @@ static void vunmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end)
 	pud = pud_offset(pgd, addr);
 	do {
 		next = pud_addr_end(addr, end);
+		if (pud_large(*pud)) {
+			pud_clear(pud);
+			continue;
+		}
 		if (pud_none_or_clear_bad(pud))
 			continue;
 		vunmap_pmd_range(pud, addr, next);
@@ -1314,7 +1322,7 @@ static struct vm_struct *__get_vm_area_node(unsigned long size,
 
 	BUG_ON(in_interrupt());
 	if (flags & VM_IOREMAP)
-		align = 1ul << clamp(fls(size), PAGE_SHIFT, IOREMAP_MAX_ORDER);
+		align = 1ul << clamp(fls64((__u64)size), PAGE_SHIFT, IOREMAP_MAX_ORDER);
 
 	size = PAGE_ALIGN(size);
 	if (unlikely(!size))
-- 
2.1.0

